<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.21.2 -->
<document source="/Users/sarinacanelake/static-clones/edx-platform/docs/concepts/testing/testing.rst" translation_progress="{'total': 0, 'translated': 0}" xmlns:c="https://www.sphinx-doc.org/" xmlns:changeset="https://www.sphinx-doc.org/" xmlns:citation="https://www.sphinx-doc.org/" xmlns:cpp="https://www.sphinx-doc.org/" xmlns:http="https://www.sphinx-doc.org/" xmlns:index="https://www.sphinx-doc.org/" xmlns:js="https://www.sphinx-doc.org/" xmlns:math="https://www.sphinx-doc.org/" xmlns:py="https://www.sphinx-doc.org/" xmlns:rst="https://www.sphinx-doc.org/" xmlns:std="https://www.sphinx-doc.org/">
    <section ids="testing" names="testing">
        <title>Testing</title>
        <topic classes="contents local" ids="contents" names="contents">
            <bullet_list>
                <list_item>
                    <paragraph><reference ids="id2" refid="overview">Overview</reference></paragraph>
                    <bullet_list>
                        <list_item>
                            <paragraph><reference ids="id3" refid="test-types">Test Types</reference></paragraph>
                            <bullet_list>
                                <list_item>
                                    <paragraph><reference ids="id4" refid="unit-tests">Unit Tests</reference></paragraph>
                                </list_item>
                                <list_item>
                                    <paragraph><reference ids="id5" refid="integration-tests">Integration Tests</reference></paragraph>
                                </list_item>
                            </bullet_list>
                        </list_item>
                        <list_item>
                            <paragraph><reference ids="id6" refid="test-locations">Test Locations</reference></paragraph>
                        </list_item>
                    </bullet_list>
                </list_item>
                <list_item>
                    <paragraph><reference ids="id7" refid="running-tests">Running Tests</reference></paragraph>
                    <bullet_list>
                        <list_item>
                            <paragraph><reference ids="id8" refid="running-python-unit-tests">Running Python Unit tests</reference></paragraph>
                            <bullet_list>
                                <list_item>
                                    <paragraph><reference ids="id9" refid="running-python-test-subsets">Running Python Test Subsets</reference></paragraph>
                                </list_item>
                                <list_item>
                                    <paragraph><reference ids="id10" refid="debugging-a-test">Debugging a test</reference></paragraph>
                                </list_item>
                                <list_item>
                                    <paragraph><reference ids="id11" refid="how-to-output-coverage-locally">How to output coverage locally</reference></paragraph>
                                </list_item>
                                <list_item>
                                    <paragraph><reference ids="id12" refid="debugging-unittest-flakiness">Debugging Unittest Flakiness</reference></paragraph>
                                </list_item>
                            </bullet_list>
                        </list_item>
                        <list_item>
                            <paragraph><reference ids="id13" refid="running-javascript-unit-tests">Running Javascript Unit Tests</reference></paragraph>
                            <bullet_list>
                                <list_item>
                                    <paragraph><reference ids="id14" refid="debugging-specific-javascript-tests">Debugging Specific Javascript Tests</reference></paragraph>
                                </list_item>
                                <list_item>
                                    <paragraph><reference ids="id15" refid="debugging-tests-in-a-browser">Debugging Tests in a Browser</reference></paragraph>
                                </list_item>
                            </bullet_list>
                        </list_item>
                        <list_item>
                            <paragraph><reference ids="id16" refid="testing-internationalization-with-dummy-translations">Testing internationalization with dummy translations</reference></paragraph>
                        </list_item>
                        <list_item>
                            <paragraph><reference ids="id17" refid="test-coverage-and-quality">Test Coverage and Quality</reference></paragraph>
                            <bullet_list>
                                <list_item>
                                    <paragraph><reference ids="id18" refid="viewing-test-coverage">Viewing Test Coverage</reference></paragraph>
                                </list_item>
                                <list_item>
                                    <paragraph><reference ids="id19" refid="python-code-style-quality">Python Code Style Quality</reference></paragraph>
                                </list_item>
                                <list_item>
                                    <paragraph><reference ids="id20" refid="javascript-code-style-quality">JavaScript Code Style Quality</reference></paragraph>
                                </list_item>
                            </bullet_list>
                        </list_item>
                    </bullet_list>
                </list_item>
                <list_item>
                    <paragraph><reference ids="id21" refid="code-complexity-tools">Code Complexity Tools</reference></paragraph>
                </list_item>
                <list_item>
                    <paragraph><reference ids="id22" refid="other-testing-tips">Other Testing Tips</reference></paragraph>
                    <bullet_list>
                        <list_item>
                            <paragraph><reference ids="id23" refid="connecting-to-browser">Connecting to Browser</reference></paragraph>
                        </list_item>
                        <list_item>
                            <paragraph><reference ids="id24" refid="factories">Factories</reference></paragraph>
                        </list_item>
                        <list_item>
                            <paragraph><reference ids="id25" refid="running-tests-on-paver-scripts">Running Tests on Paver Scripts</reference></paragraph>
                        </list_item>
                        <list_item>
                            <paragraph><reference ids="id26" refid="testing-using-queue-servers">Testing using queue servers</reference></paragraph>
                        </list_item>
                    </bullet_list>
                </list_item>
            </bullet_list>
        </topic>
        <section ids="overview" names="overview">
            <title refid="id2">Overview</title>
            <paragraph>We maintain two kinds of tests: unit tests and integration tests.</paragraph>
            <paragraph>Overall, you want to write the tests that <strong>maximize coverage</strong> while
                <strong>minimizing maintenance</strong>. In practice, this usually means investing
                heavily in unit tests, which tend to be the most robust to changes in
                the code base.</paragraph>
            <figure ids="id1">
                <image alt="Test Pyramid" candidates="{'*': 'concepts/testing/test_pyramid.png'}" original_uri="test_pyramid.png" uri="concepts/testing/test_pyramid.png"></image>
                <caption>Test Pyramid</caption>
            </figure>
            <paragraph>The pyramid above shows the relative number of unit tests and integration
                tests. Most of our tests are unit tests or
                integration tests.</paragraph>
            <section ids="test-types" names="test\ types">
                <title refid="id3">Test Types</title>
                <section ids="unit-tests" names="unit\ tests">
                    <title refid="id4">Unit Tests</title>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph>Each test case should be concise: setup, execute, check, and
                                teardown. If you find yourself writing tests with many steps,
                                consider refactoring the unit under tests into smaller units, and
                                then testing those individually.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>As a rule of thumb, your unit tests should cover every code branch.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>Mock or patch external dependencies. We use the voidspace <reference name="Mock Library" refuri="http://www.voidspace.org.uk/python/mock/">Mock Library</reference>.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>We unit test Python code (using <reference name="unittest" refuri="http://docs.python.org/2/library/unittest.html">unittest</reference>) and Javascript (using
                                <reference name="Jasmine" refuri="http://jasmine.github.io/">Jasmine</reference>)</paragraph>
                        </list_item>
                    </bullet_list>
                    <target ids="mock-library" names="mock\ library" refuri="http://www.voidspace.org.uk/python/mock/"></target>
                    <target ids="unittest" names="unittest" refuri="http://docs.python.org/2/library/unittest.html"></target>
                    <target ids="jasmine" names="jasmine" refuri="http://jasmine.github.io/"></target>
                </section>
                <section ids="integration-tests" names="integration\ tests">
                    <title refid="id5">Integration Tests</title>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph>Test several units at the same time. Note that you can still mock or patch
                                dependencies that are not under test! For example, you might test that
                                <literal>LoncapaProblem</literal>, <literal>NumericalResponse</literal>, and <literal>CorrectMap</literal> in the <literal>capa</literal>
                                package work together, while still mocking out template rendering.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>Use integration tests to ensure that units are hooked up correctly.  You do
                                not need to test every possible input–that’s what unit tests are for.
                                Instead, focus on testing the “happy path” to verify that the components work
                                together correctly.</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>Many of our tests use the <reference name="Django test client" refuri="https://docs.djangoproject.com/en/dev/topics/testing/overview/">Django test client</reference> to simulate HTTP requests to
                                the server.</paragraph>
                        </list_item>
                    </bullet_list>
                    <target ids="django-test-client" names="django\ test\ client" refuri="https://docs.djangoproject.com/en/dev/topics/testing/overview/"></target>
                </section>
            </section>
            <section ids="test-locations" names="test\ locations">
                <title refid="id6">Test Locations</title>
                <bullet_list bullet="-">
                    <list_item>
                        <paragraph>Python unit and integration tests: Located in subpackages called
                            <literal>tests</literal>. For example, the tests for the <literal>capa</literal> package are
                            located in <literal>xmodule/capa/tests</literal>.</paragraph>
                    </list_item>
                    <list_item>
                        <paragraph>Javascript unit tests: Located in <literal>spec</literal> folders. For example,
                            <literal>xmodule/js/spec</literal> and
                            <literal>{cms,lms}/static/js/spec</literal> For consistency, you should use the
                            same directory structure for implementation and test. For example,
                            the test for <literal>src/views/module.js</literal> should be written in
                            <literal>spec/views/module_spec.js</literal>.</paragraph>
                    </list_item>
                </bullet_list>
            </section>
        </section>
        <section ids="running-tests" names="running\ tests">
            <title refid="id7">Running Tests</title>
            <paragraph><strong>Unless otherwise mentioned, all the following commands should be run from inside the lms docker container.</strong></paragraph>
            <section ids="running-python-unit-tests" names="running\ python\ unit\ tests">
                <title refid="id8">Running Python Unit tests</title>
                <paragraph>We use <reference name="pytest" refuri="https://pytest.org/">pytest</reference> to run Python tests. Pytest is a testing framework for python and should be your goto for local Python unit testing.</paragraph>
                <paragraph>Pytest (and all of the plugins we use with it) has a lot of options. Use <title_reference>pytest –help</title_reference> to see all your option and pytest has good docs around testing.</paragraph>
                <target ids="pytest" names="pytest" refuri="https://pytest.org/"></target>
                <section ids="running-python-test-subsets" names="running\ python\ test\ subsets">
                    <title refid="id9">Running Python Test Subsets</title>
                    <paragraph>When developing tests, it is often helpful to be able to really just run one single test without the overhead of PIP installs, UX builds, etc.</paragraph>
                    <paragraph>Various ways to run tests using pytest:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest path/test_m­odule.py                          # Run all tests in a module.
pytest path/test_m­odule.p­y:­:te­st_func               # Run a specific test within a module.
pytest path/test_m­odule.p­y:­:Te­stC­las­s               # Run all tests in a class
pytest path/test_m­odule.p­y:­:Te­stC­las­s::­tes­t_m­ethod  # Run a specific method of a class.
pytest path/testing/                                # Run all tests in a directory.</literal_block>
                    <paragraph>For example, this command runs a single python unit test file:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest xmodule/tests/test_stringify.py</literal_block>
                    <paragraph>Note -
                        edx-platorm has multiple services (lms, cms) in it. The environment for each service is different enough that we run some tests in both environments in Github Actions.
                        To test in each of these environments (especially for tests in “common” and “xmodule” directories), you will need to test in each seperately.
                        To specify that the tests are run with the relevant service as root, Add –rootdir flag at end of your pytest call and specify the env to test in:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest test --rootdir &lt;lms or cms&gt;</literal_block>
                    <paragraph>Or, if you need django settings from a particular enviroment, add –ds flag to the end of your pytest call and specify the django settings object:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest test --ds=&lt;lms.envs.test or cms.envs.test&gt;</literal_block>
                    <paragraph>Various tools like ddt create tests with very complex names, rather than figuring out the name yourself, you can:</paragraph>
                    <enumerated_list enumtype="arabic" prefix="" suffix=".">
                        <list_item>
                            <paragraph>Select tests to run based on their name, provide an expression to the <reference name="pytest -k option" refuri="https://docs.pytest.org/en/latest/example/markers.html#using-k-expr-to-select-tests-based-on-their-name">pytest -k option</reference> which performs a substring match on test names:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest xmodule/tests/test_stringify.py -k test_stringify</literal_block>
                        </list_item>
                    </enumerated_list>
                    <target ids="pytest-k-option" names="pytest\ -k\ option" refuri="https://docs.pytest.org/en/latest/example/markers.html#using-k-expr-to-select-tests-based-on-their-name"></target>
                    <target ids="node-id" names="node\ id" refuri="https://docs.pytest.org/en/latest/example/markers.html#node-id"></target>
                    <enumerated_list enumtype="arabic" prefix="" start="2" suffix=".">
                        <list_item>
                            <paragraph>Alternatively, you can the get the name of all test methods in a class, file, or project, including all ddt.data variations, by running pytest with <title_reference>–collectonly</title_reference>:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest xmodule/tests/test_stringify.py --collectonly</literal_block>
                        </list_item>
                    </enumerated_list>
                    <section ids="testing-with-migrations" names="testing\ with\ migrations">
                        <title>Testing with migrations</title>
                        <paragraph>For the sake of speed, by default the python unit test database tables
                            are created directly from apps’ models. If you want to run the tests
                            against a database created by applying the migrations instead, use the
                            <literal>--create-db --migrations</literal> option:</paragraph>
                        <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest test --create-db --migrations</literal_block>
                    </section>
                </section>
                <section ids="debugging-a-test" names="debugging\ a\ test">
                    <title refid="id10">Debugging a test</title>
                    <paragraph>There are various ways to debug tests in Python and more specifically with pytest:</paragraph>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph>using the verbose -v or really verbose -vv flags can be helpful for displaying diffs on assertion failures</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>if you want to focus on one test failure at a time, the <literal>--exitfirst``or ``-x</literal> flags to have pytest stop after the first failure</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>by default, the plugin pytest-randomly will randomize test case sequence. This is to help reveal bugs in your test setup and teardown. If you do not want this randomness, use the –randomly-dont-reorganize flag</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>if you pass the <literal>--pdb</literal> flag to a pytest call, the test runner will drop you into pdb on error. This lets you go up and down the stack and see what the values of the variables are. Check out <reference name="the pdb documentation" refuri="http://docs.python.org/library/pdb.html">the pdb documentation</reference>.  Note that this only works if you aren’t collecting coverage statistics (pdb and coverage.py use the same mechanism to trace code execution).</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>If there is a specific point in code you would like to debug, you can add the build-in “breakpoint()” function there and it will automatically drop you at the point next time the code runs. If you check this in, your tests will hang on jenkins. Example of use:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">if True:
  # you will be dropped here in the pdb shell when running test or code
  breakpoint()
  a=2
  random_variable = False</literal_block>
                        </list_item>
                    </bullet_list>
                    <target ids="the-pdb-documentation" names="the\ pdb\ documentation" refuri="http://docs.python.org/library/pdb.html"></target>
                </section>
                <section ids="how-to-output-coverage-locally" names="how\ to\ output\ coverage\ locally">
                    <title refid="id11">How to output coverage locally</title>
                    <paragraph>These are examples of how to run a single test and get coverage:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest cms/djangoapps/contentstore/tests/test_import.py --cov --cov-config=.coveragerc-local # cms example
pytest lms/djangoapps/courseware/tests/test_block_render.py --cov --cov-config=.coveragerc-local # lms example</literal_block>
                    <paragraph>That <literal>--cov-conifg=.coveragerc-local</literal> option is important - without it, the coverage
                        tool will look for paths that exist on our jenkins test servers, but not on your local devstack.</paragraph>
                    <paragraph>How to spit out coverage for a single file with a list of each line that is missing coverage:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest lms/djangoapps/grades/tests/test_subsection_grade.py \
    --cov=lms.djangoapps.grades.subsection_grade \
    --cov-config=.coveragerc-local \
    --cov-report=term-missing
---------- coverage: platform linux2, python 2.7.12-final-0 ----------

Name                                        Stmts   Miss  Cover   Missing
-------------------------------------------------------------------------
lms/djangoapps/grades/subsection_grade.py     125     38    70%   47-51, 57, 80-81, 85, 89, 99, 109, 113, [...]</literal_block>
                    <paragraph>Use this command to generate a coverage report (after previously running <literal>pytest</literal>):</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">coverage report</literal_block>
                    <paragraph>The above command looks for a test coverage data file in <literal>reports/.coverage</literal> - this file will
                        contain coverage data from your last run of <literal>pytest</literal>.  Coverage data is recorded for whichever
                        paths you specified in your <literal>--cov</literal> option, e.g.:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">--cov=.  # will track coverage for the entire project
--cov=path.to.your.module  # will track coverage only for "module"</literal_block>
                    <paragraph>Use this command to generate an HTML report:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">coverage html</literal_block>
                    <paragraph>The report is then saved in reports/xmodule/cover/index.html</paragraph>
                    <paragraph>To run tests for stub servers, for example for <reference name="YouTube stub server" refuri="https://github.com/openedx/edx-platform/blob/master/common/djangoapps/terrain/stubs/tests/test_youtube_stub.py">YouTube stub server</reference>, you can
                        run one of these commands:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest --ds=cms.env.test common/djangoapps/terrain/stubs/tests/test_youtube_stub.py</literal_block>
                    <target ids="youtube-stub-server" names="youtube\ stub\ server" refuri="https://github.com/openedx/edx-platform/blob/master/common/djangoapps/terrain/stubs/tests/test_youtube_stub.py"></target>
                </section>
                <section ids="debugging-unittest-flakiness" names="debugging\ unittest\ flakiness">
                    <title refid="id12">Debugging Unittest Flakiness</title>
                    <paragraph>As we move over to running our unittests with Jenkins Pipelines and pytest-xdist,
                        there are new ways for tests to flake, which can sometimes be difficult to debug.
                        If you run into flakiness, check (and feel free to contribute to) this
                        <reference name="confluence document" refuri="https://openedx.atlassian.net/wiki/spaces/TE/pages/884998163/Debugging+test+failures+with+pytest-xdist">confluence document</reference> for help.</paragraph>
                </section>
            </section>
            <section ids="running-javascript-unit-tests" names="running\ javascript\ unit\ tests">
                <title refid="id13">Running Javascript Unit Tests</title>
                <paragraph>Before running Javascript unit tests, you will need to be running Firefox or Chrome in a place visible to edx-platform. If running this in devstack, you can run <literal>make dev.up.firefox</literal> or <literal>make dev.up.chrome</literal>. Firefox is the default browser for the tests, so if you decide to use Chrome, you will need to prefix the test command with <literal>SELENIUM_BROWSER=chrome SELENIUM_HOST=edx.devstack.chrome</literal> (if using devstack).</paragraph>
                <paragraph>We use Jasmine to run JavaScript unit tests. To run all the JavaScript
                    tests:</paragraph>
                <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver test_js</literal_block>
                <paragraph>To run a specific set of JavaScript tests and print the results to the
                    console, run these commands:</paragraph>
                <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver test_js_run -s lms
paver test_js_run -s cms
paver test_js_run -s cms-squire
paver test_js_run -s xmodule
paver test_js_run -s xmodule-webpack
paver test_js_run -s common
paver test_js_run -s common-requirejs</literal_block>
                <paragraph>To run JavaScript tests in a browser, run these commands:</paragraph>
                <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver test_js_dev -s lms
paver test_js_dev -s cms
paver test_js_dev -s cms-squire
paver test_js_dev -s xmodule
paver test_js_dev -s xmodule-webpack
paver test_js_dev -s common
paver test_js_dev -s common-requirejs</literal_block>
                <section ids="debugging-specific-javascript-tests" names="debugging\ specific\ javascript\ tests">
                    <title refid="id14">Debugging Specific Javascript Tests</title>
                    <paragraph>The best way to debug individual tests is to run the test suite in the browser and
                        use your browser’s Javascript debugger. The debug page will allow you to select
                        an individual test and only view the results of that test.</paragraph>
                </section>
                <section ids="debugging-tests-in-a-browser" names="debugging\ tests\ in\ a\ browser">
                    <title refid="id15">Debugging Tests in a Browser</title>
                    <paragraph>To debug these tests on devstack in a local browser:</paragraph>
                    <bullet_list bullet="*">
                        <list_item>
                            <paragraph>first run the appropriate test_js_dev command from above</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>open <reference refuri="http://localhost:19876/debug.html">http://localhost:19876/debug.html</reference> in your host system’s browser of choice</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>this will run all the tests and show you the results including details of any failures</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>you can click on an individually failing test and/or suite to re-run it by itself</paragraph>
                        </list_item>
                        <list_item>
                            <paragraph>you can now use the browser’s developer tools to debug as you would any other JavaScript code</paragraph>
                        </list_item>
                    </bullet_list>
                    <paragraph>Note: the port is also output to the console that you ran the tests from if you find that easier.</paragraph>
                    <paragraph>These paver commands call through to Karma. For more
                        info, see <reference name="karma-runner.github.io" refuri="https://karma-runner.github.io/">karma-runner.github.io</reference>.</paragraph>
                </section>
            </section>
            <section ids="testing-internationalization-with-dummy-translations" names="testing\ internationalization\ with\ dummy\ translations">
                <title refid="id16">Testing internationalization with dummy translations</title>
                <paragraph>Any text you add to the platform should be internationalized. To generate translations for your new strings, run the following command:</paragraph>
                <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver i18n_dummy</literal_block>
                <paragraph>This command generates dummy translations for each dummy language in the
                    platform and puts the dummy strings in the appropriate language files.
                    You can then preview the dummy languages on your local machine and also in your sandbox, if and when you create one.</paragraph>
                <paragraph>The dummy language files that are generated during this process can be
                    found in the following locations:</paragraph>
                <literal_block force="False" language="default" linenos="False" xml:space="preserve">conf/locale/{LANG_CODE}</literal_block>
                <paragraph>There are a few JavaScript files that are generated from this process. You can find those in the following locations:</paragraph>
                <literal_block force="False" language="default" linenos="False" xml:space="preserve">lms/static/js/i18n/{LANG_CODE}
cms/static/js/i18n/{LANG_CODE}</literal_block>
                <paragraph>Do not commit the <literal>.po</literal>, <literal>.mo</literal>, <literal>.js</literal> files that are generated
                    in the above locations during the dummy translation process!</paragraph>
            </section>
            <section ids="test-coverage-and-quality" names="test\ coverage\ and\ quality">
                <title refid="id17">Test Coverage and Quality</title>
                <section ids="viewing-test-coverage" names="viewing\ test\ coverage">
                    <title refid="id18">Viewing Test Coverage</title>
                    <paragraph>We currently collect test coverage information for Python
                        unit/integration tests.</paragraph>
                    <paragraph>To view test coverage:</paragraph>
                    <enumerated_list enumtype="arabic" prefix="" suffix=".">
                        <list_item>
                            <paragraph>Run the test suite with this command:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver test</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>Generate reports with this command:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver coverage</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>Reports are located in the <literal>reports</literal> folder. The command generates
                                HTML and XML (Cobertura format) reports.</paragraph>
                        </list_item>
                    </enumerated_list>
                </section>
                <section ids="python-code-style-quality" names="python\ code\ style\ quality">
                    <title refid="id19">Python Code Style Quality</title>
                    <paragraph>To view Python code style quality (including PEP 8 and pylint violations) run this command:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver run_quality</literal_block>
                    <paragraph>More specific options are below.</paragraph>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph>These commands run a particular quality report:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver run_pep8
paver run_pylint</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>This command runs a report, and sets it to fail if it exceeds a given number
                                of violations:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver run_pep8 --limit=800</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>The <literal>run_quality</literal> uses the underlying diff-quality tool (which is packaged
                                with <reference name="diff-cover" refuri="https://github.com/Bachmann1234/diff-cover">diff-cover</reference>). With that, the command can be set to fail if a certain
                                diff threshold is not met. For example, to cause the process to fail if
                                quality expectations are less than 100% when compared to master (or in other
                                words, if style quality is worse than what is already on master):</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver run_quality --percentage=100</literal_block>
                        </list_item>
                        <list_item>
                            <paragraph>Note that ‘fixme’ violations are not counted with run_quality. To
                                see all ‘TODO’ lines, use this command:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver find_fixme --system=lms</literal_block>
                            <paragraph><literal>system</literal> is an optional argument here. It defaults to
                                <literal>cms,lms,common</literal>.</paragraph>
                        </list_item>
                    </bullet_list>
                    <target ids="diff-cover" names="diff-cover" refuri="https://github.com/Bachmann1234/diff-cover"></target>
                </section>
                <section ids="javascript-code-style-quality" names="javascript\ code\ style\ quality">
                    <title refid="id20">JavaScript Code Style Quality</title>
                    <paragraph>To view JavaScript code style quality run this command:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver run_eslint</literal_block>
                    <bullet_list bullet="-">
                        <list_item>
                            <paragraph>This command also comes with a <literal>--limit</literal> switch, this is an example of that switch:</paragraph>
                            <literal_block force="False" language="default" linenos="False" xml:space="preserve">paver run_eslint --limit=50000</literal_block>
                        </list_item>
                    </bullet_list>
                </section>
            </section>
        </section>
        <section ids="code-complexity-tools" names="code\ complexity\ tools">
            <title refid="id21">Code Complexity Tools</title>
            <paragraph>Tool(s) available for evaluating complexity of edx-platform code:</paragraph>
            <bullet_list bullet="-">
                <list_item>
                    <paragraph><reference name="plato" refuri="https://github.com/es-analysis/plato">plato</reference> for JavaScript code
                        complexity. Several options are available on the command line; see
                        documentation.  Below, the following command will produce an HTML report in a
                        subdirectory called “jscomplexity”:</paragraph>
                    <literal_block force="False" language="default" linenos="False" xml:space="preserve">plato -q -x common/static/js/vendor/ -t common -e .eslintrc.json -r -d jscomplexity common/static/js/</literal_block>
                </list_item>
            </bullet_list>
        </section>
        <section ids="other-testing-tips" names="other\ testing\ tips">
            <title refid="id22">Other Testing Tips</title>
            <section ids="connecting-to-browser" names="connecting\ to\ browser">
                <title refid="id23">Connecting to Browser</title>
                <paragraph>If you want to see the browser being automated for JavaScript,
                    you can connect to the container running it via VNC.</paragraph>
                <table>
                    <tgroup cols="2">
                        <colspec colwidth="24"></colspec>
                        <colspec colwidth="22"></colspec>
                        <thead>
                            <row>
                                <entry>
                                    <paragraph>Browser</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>VNC connection</paragraph>
                                </entry>
                            </row>
                        </thead>
                        <tbody>
                            <row>
                                <entry>
                                    <paragraph>Firefox (Default)</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>vnc://0.0.0.0:25900</paragraph>
                                </entry>
                            </row>
                            <row>
                                <entry>
                                    <paragraph>Chrome (via Selenium)</paragraph>
                                </entry>
                                <entry>
                                    <paragraph>vnc://0.0.0.0:15900</paragraph>
                                </entry>
                            </row>
                        </tbody>
                    </tgroup>
                </table>
                <paragraph>On macOS, enter the VNC connection string in Safari to connect via VNC. The VNC
                    passwords for both browsers are randomly generated and logged at container
                    startup, and can be found by running <literal>make vnc-passwords</literal>.</paragraph>
                <paragraph>Most tests are run in Firefox by default.  To use Chrome for tests that normally
                    use Firefox instead, prefix the test command with
                    <literal>SELENIUM_BROWSER=chrome SELENIUM_HOST=edx.devstack.chrome</literal></paragraph>
            </section>
            <section ids="factories" names="factories">
                <title refid="id24">Factories</title>
                <paragraph>Many tests delegate set-up to a “factory” class. For example, there are
                    factories for creating courses, problems, and users. This encapsulates
                    set-up logic from tests.</paragraph>
                <paragraph>Factories are often implemented using <reference name="FactoryBoy" refuri="https://readthedocs.org/projects/factoryboy/">FactoryBoy</reference>.</paragraph>
                <paragraph>In general, factories should be located close to the code they use. For
                    example, the factory for creating problem XML definitions is located in
                    <literal>xmodule/capa/tests/response_xml_factory.py</literal> because the
                    <literal>capa</literal> package handles problem XML.</paragraph>
                <target ids="factoryboy" names="factoryboy" refuri="https://readthedocs.org/projects/factoryboy/"></target>
            </section>
            <section ids="running-tests-on-paver-scripts" names="running\ tests\ on\ paver\ scripts">
                <title refid="id25">Running Tests on Paver Scripts</title>
                <paragraph>To run tests on the scripts that power the various Paver commands, use the following command:</paragraph>
                <literal_block force="False" language="default" linenos="False" xml:space="preserve">pytest pavelib</literal_block>
            </section>
            <section ids="testing-using-queue-servers" names="testing\ using\ queue\ servers">
                <title refid="id26">Testing using queue servers</title>
                <paragraph>When testing problems that use a queue server on AWS (e.g.
                    sandbox-xqueue.edx.org), you’ll need to run your server on your public IP, like so:</paragraph>
                <literal_block force="False" language="default" linenos="False" xml:space="preserve">./manage.py lms runserver 0.0.0.0:8000</literal_block>
                <paragraph>When you connect to the LMS, you need to use the public ip. Use
                    <literal>ifconfig</literal> to figure out the number, and connect e.g. to
                    <literal>http://18.3.4.5:8000/</literal></paragraph>
            </section>
        </section>
    </section>
</document>
